{"meta":{"title":"Zhaoheng Ni","subtitle":null,"description":"Zhaoheng Ni's blog","author":"Zhaoheng Ni","url":"nateanl.github.io","root":"/"},"pages":[{"title":"About","date":"2020-09-01T16:02:54.959Z","updated":"2020-09-01T16:02:54.959Z","comments":true,"path":"about/index.html","permalink":"nateanl.github.io/about/index.html","excerpt":"","text":""},{"title":"Project","date":"2020-03-04T08:59:48.449Z","updated":"2020-03-04T08:59:48.449Z","comments":true,"path":"project/index.html","permalink":"nateanl.github.io/project/index.html","excerpt":"","text":""},{"title":"Tags","date":"2020-03-04T08:59:48.450Z","updated":"2020-03-04T08:59:48.450Z","comments":true,"path":"tags/index.html","permalink":"nateanl.github.io/tags/index.html","excerpt":"","text":""},{"title":"","date":"2020-04-22T07:02:28.553Z","updated":"2019-08-26T00:04:31.784Z","comments":false,"path":"publications/index.html","permalink":"nateanl.github.io/publications/index.html","excerpt":"","text":"Zhaoheng Ni, Rutuja Ubale, Yao Qian, Michael Mandel, Su-Youn Yoon, Abhinav Misra, and David Suendermann-Oeft. “Unusable Spoken Response Detection with BLSTM Neural Networks.” ISCSLP 2018 Weicheng Ma, Kai Cao, Zhaoheng Ni, Peter Chin, and Xiang Li. “Sound Signal Processing with Seq2Tree Network.” In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018). 2018. Zhaoheng Ni, Ahmet Cem Yuksel, Xiuyan Ni, Michael I. Mandel, and Lei Xie. “Confused or not confused?: Disentangling brain activity from eeg data using bidirectional lstm recurrent neural networks.” In Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics, pp. 241-246. ACM, 2017. Weicheng Ma, Kai Cao, Zhaoheng Ni, Xiuyan Ni, and Sang Chin. “Sound signal processing based on seq2tree network.” In Proceedings of Interspeech workshop on Vocal Interactivity in-andbetween Humans, Animals and Robots. 2017. Yan Xu, Ji Hua, Zhaoheng Ni, Qinlang Chen, Yubo Fan, Sophia Ananiadou, I. Eric, Chao Chang, and Junichi Tsujii. “Anatomical entity recognition with a hierarchical framework augmented by external resources.” PloS one 9, no. 10 (2014): e108396."}],"posts":[{"title":"WPD++: An Improved Neural Beamformer for Simultaneous Speech Separation and Dereverberation","slug":"wpd++","date":"2020-11-05T02:10:00.000Z","updated":"2020-11-08T03:28:58.361Z","comments":true,"path":"2020/11/05/wpd++/","link":"","permalink":"nateanl.github.io/2020/11/05/wpd++/","excerpt":"","text":"Accepted by SLT 2021 Zhaoheng Ni (zni@gradcenter.cuny.edu), Yong Xu, Meng Yu, Bo Wu, Shixiong Zhang, Dong Yu, Michael I Mandel This paper aims at eliminating the interfering speakers’ speech, additive noise, and reverberation from the noisy multi-talker speech mixture that benefits automatic speech recognition (ASR) backend. While the recently proposed Weighted Power minimization Distortionless response (WPD) beamformer can perform separation and dereverberation simultaneously, the noise cancellation component still has the potential to progress. We propose an improved neural WPD beamformer called “WPD++” by an enhanced beamforming module in the conventional WPD and a multi-objective loss function for the joint training. The beamforming module is improved by utilizing the spatio-temporal correlation. A multi-objective loss, including the complex spectra domain scale-invariant signal-to-noise ratio (C-Si-SNR) and the magnitude domain mean square error (Mag-MSE), is properly designed to make multiple constraints on the enhanced speech and the desired power of the dry clean signal. Joint training is conducted to optimize the complex-valued mask estimator and the WPD++ beamformer in an end-to-end way. The results show that the proposed WPD++ outperforms several state-of-the-art beamformers on the enhanced speech quality and word error rate (WER) of ASR. A Mandarin audio-visual dataset [1,2] is adopted for this study. Systems evaluated: Multi-tap MVDR with cRM: A multi-tap MVDR system with complex ratio mask (cRM) [1] WPD with cRM [3,4] The proposed WPD++ All systems use the sane cRM estimation network that is a Conv-TasNet variant [5] with complex ratio mask Demo 1: Simulated 1-speaker noisy mixture for target speech separation [Sorry that all the demos are recorded in Mandarin Chinese.] Mix (1 speaker + non-stationary additive noise Reverberant clean (reference) Dry clean (reference) Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. &nbsp;&nbsp; Multi-tap MVDR with cRM WPD with cRM Proposed WPD++ Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. &nbsp;&nbsp; Demo 2: Simulated 2-speaker noisy mixture for target speech separation separation Mix (2 speakers + non-stationary additive noise Reverberant clean (reference) Dry clean (reference) Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. &nbsp;&nbsp; Multi-tap MVDR with cRM WPD with cRM Proposed WPD++ Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. &nbsp;&nbsp; Demo 3: Simulated 3-speaker noisy mixture for target speech separation Mix (3 speakers + non-stationary additive noise Reverberant clean (reference) Dry clean (reference) Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. &nbsp;&nbsp; Multi-tap MVDR with cRM WPD with cRM Proposed WPD++ Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. &nbsp;&nbsp; Real-world scenario: far-field recording and testing: Real-world recording hardware device: 15-element non-uniform linear microphone array and 180 degree wide-angle camera For the real-world videos, the 180-degree wide-angle camera is calibrated and synchronized with the 15-channel mic array. We estimate the rough DOA of the target speaker according to the location of the target speaker in the whole camera view [4]. Face detection is applied to track the target speaker's DOA. Demo 4: Real-world far-field recording and testing 1: Real-world scenario: far-field two-speaker mixture recorded by the camera and microphone array Real-world scenario: separated male voice by the multi-tap MVDR method. Real-world scenario: separated male voice by the WPD method. Real-world scenario: separated male voice by the proposed WPD++ method (face detected in the red rectangle, used for DOA estimation). Demo 5: Real-world far-field recording and testing 2: Real-world scenario: far-field two-speaker mixture recorded by the camera and microphone array Real-world scenario: separated female voice by the multi-tap MVDR method. Real-world scenario: separated female voice by the WPD method. Real-world scenario: separated female voice by the proposed WPD++ method (face detected in the red rectangle, used for DOA estimation). &nbsp; Reference: [1] Xu, Yong, et al. \"Neural Spatio-Temporal Beamformer for Target Speech Separation.\" accepted to Interspeech2020. [2] Tan, Ke, et al. \"Audio-visual speech separation and dereverberation with a two-stage multimodal network.\" IEEE Journal of Selected Topics in Signal Processing (2020). [3] Nakatani Tomohiro, et al. \"A unified convolutional beamformer for simultaneous denoising and dereverberation.\" IEEE Signal Processing Letters, 2019 [4] Zhang, Wangyou, et al. \"End-to-End Far-Field Speech Recognition with Unified Dereverberation and Beamforming.\" arXiv preprint arXiv:2005.10479 (2020). [5] Luo, Yi, and Nima Mesgarani. \"Conv-tasnet: Surpassing ideal time–frequency magnitude masking for speech separation.\" IEEE/ACM transactions on audio, speech, and language processing 27.8 (2019): 1256-1266. &nbsp;","categories":[],"tags":[{"name":"dnn, asr, speech separation","slug":"dnn-asr-speech-separation","permalink":"nateanl.github.io/tags/dnn-asr-speech-separation/"}]},{"title":"From forced alignment to acoustic model: a chicken-and-egg cycle in ASR","slug":"force-aligment","date":"2020-09-01T14:25:00.000Z","updated":"2020-09-02T01:01:54.821Z","comments":true,"path":"2020/09/01/force-aligment/","link":"","permalink":"nateanl.github.io/2020/09/01/force-aligment/","excerpt":"","text":"If you are familiar with Kaldi, an open-source toolkit for speech recognition, then you must know those crazy script shits like this: steps/train_mono.sh # mono-phone AM...steps/align_si.sh...steps/train_lda_mllt.sh # tri-phone AM...steps/align_si.sh...steps/train_sat.sh # tri-phone AM again Wow, why are there so many training and aligning loops, like it never stops? If the ASR system is not an end-to-end system (i.e. from wav to text), it usually comprises an acoustic model (AM), which predicts the acoustic state of each frame, and a language model (LM), which predicts the word sequence out of the acoustic states. The acoustic model could be of any kind, for example, GMM, HMM, or DNN. But to train the acoustic model, we need to prepare the feature and label pairs. If the feature is MFCC, the label is the phonemes for the frames. Here comes the problem, not all frames contains speech. Many of them have only noises, or silence. We need to correctly align the labels and the corresponding frames. Do we have such accurate labels? No. We usually have the text label annotated by humans, however, they are usually in a sentence level and don’t have the precise start and end points for each phoneme. So, what should we do? The first ASR genius who thought of the above scripts might say: Fxxk it, let’s just train it. I have the MFCC frames, and I have the phonemes, I just equally split the frames to each phoneme and feed them to the model, period. Though the labels are not so clean, the model does learn something, which means, given a frame, the model can tell which phoneme has the highest likelihood, though it’s not always accurate. Then the ASR genius said: wait, I can use this trained AM to do something. If it predicts the likelihood for all frames, why don’t I use the predictions to re-align the frames, and train another AM? So he did it. He aligns the frames based on the AM predictions and the text transcription. It is called “forced alignment”. Based on the pre-trained mono-phone AM, he generates new alignment labels and trains a tri-phone AM. The tri-phone AM is more powerful than the mono-phone AM. That’s good. But the genius is not satisfied yet. Since we have a better AM, we can get better alignment results. Why not run another round of training, and get a better AM? So you can see the trick here. If we want a better acoustic model, we need to prepare better alignments, but if we want better alignments, we need to train a better acoustic model. It’s a chicken-and-egg problem! But don’t worry, once we have a start (the mono AM), we are almost succeeding. We just keep looping the training and aligning process, like the EM algorithm. We have talked about the relationship between forced alignment and acoustic model training. Force alignment is very useful especially in the multi-talker scenario. Now I want to ask you a question: What if we have nothing but the wav files? Next time, I will talk about the methods to solve this problem…","categories":[],"tags":[{"name":"asr","slug":"asr","permalink":"nateanl.github.io/tags/asr/"}]},{"title":"Onssen: an open-source speech separation and enhancement library","slug":"onssen","date":"2020-03-05T13:52:16.000Z","updated":"2020-03-05T14:02:34.620Z","comments":true,"path":"2020/03/05/onssen/","link":"","permalink":"nateanl.github.io/2020/03/05/onssen/","excerpt":"","text":"We are happy to release onssen (pronounced as 温泉, the Japanese spring), an PyTorch-based deep learning library for speech enhancement and speech separation. Feel free to fork it on GitHub and add your models to it :) It supports the following separation models: Deep Clustering Chimera Net Chimera++ Phase Estimation Network Speech Enhancement with Restoration Layers Supported Dataset Wsj0-2mix (http://www.merl.com/demos/deep-clustering) Daps (https://archive.org/details/daps_dataset) Edinburgh-TTS (https://datashare.is.ed.ac.uk/handle/10283/2791) Requirements PyTorch LibRosa NumPy UsageYou can simply use the existing config JSON file or customize your config file to train the enhancement or separation model. 1python train.py -c configs/dc_config.json CitingIf you plan to use onssen for your research project, please cite one of the following bibtex citations: @inproceedings {onssen, author = {Zhaoheng Ni and Michael Mandel}, title = &quot;ONSSEN: An Open-source Speech Separation and Enhancement Library&quot;, publisher = &quot;under review&quot;, year = 2019 } @Misc{onssen, author = {Zhaoheng Ni and Michael Mandel}, title = &quot;ONSSEN: An Open-source Speech Separation and Enhancement Library&quot;, howpublished = {\\url{https://github.com/speechLabBcCuny/onssen}}, year = {2019} }","categories":[],"tags":[{"name":"dnn","slug":"dnn","permalink":"nateanl.github.io/tags/dnn/"}]},{"title":"Mask-dependent phase estimation for monaural speaker separation","slug":"speech-separation-demo","date":"2019-04-25T14:06:25.000Z","updated":"2020-11-05T00:22:23.106Z","comments":true,"path":"2019/04/25/speech-separation-demo/","link":"","permalink":"nateanl.github.io/2019/04/25/speech-separation-demo/","excerpt":"Speech separation refers to the task of isolating speech of interest in a multi-talker environment.","text":"Speech separation refers to the task of isolating speech of interest in a multi-talker environment. Most methods apply real-valued Time-Frequency (T-F) Masks to the Short-time Fourier Transform (STFT) of the noisy speech to get the estimated clean speech. The Idea Ratio Mask (IRM) is defined as IRM = \\frac{|S|}{|X|},where $|S|$ and $|Y|$ is the magnitude of clean speech and noisy speech respectively. The mask value is close to 1 if the T-F bin is dominated by the speaker, 0 otherwise. Since the clean STFT estimate is constructed by $IRM \\odot X$, there exists unavoidable phase difference in the masking method. We propose a novel phase estimation method which estimates the phase based on the T-F mask. Now the code is included in the onssen library. To evaluate the separation performance of our method, we compare our results with recent published ones including: Deep Clustering Chimera++ Network with MSA loss function Chimera++ Network with tPSA loss function Here are the separation demos for different methods. The testing utterance is from WSJ0-2mix dataset. Mixture Utterance tt/mix/050a0508_1.4465_22gc0102_-1.4465.wav Separation Results of different models(Swipe left to listen to the audios) Model Average SDRSpeaker 1 Speaker 2 Deep Clustering 9.2 dB Chimera++ MSA 10.2 dB Chimera++ tPSA 10.3 dB Phase Estimation 13.6 dB Clean Reference 301 dB (basically $+\\infty$) ReferencesHershey, John R., Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe. “Deep clustering: Discriminative embeddings for segmentation and separation.” In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 31-35. IEEE, 2016.Yu, Dong, Morten Kolbæk, Zheng-Hua Tan, and Jesper Jensen. “Permutation invariant training of deep models for speaker-independent multi-talker speech separation.” In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 241-245. IEEE, 2017.Wang, Zhong-Qiu, Jonathan Le Roux, and John R. Hershey. “Alternative objective functions for deep clustering.” In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 686-690. IEEE, 2018.Wang, Zhong-Qiu, Jonathan Le Roux, DeLiang Wang, and John R. Hershey. “End-to-end speech separation with unfolded iterative phase reconstruction.” arXiv preprint arXiv:1804.10204 (2018).Paszke, Adam, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. “Automatic differentiation in pytorch.” (2017).Raffel, Colin, Brian McFee, Eric J. Humphrey, Justin Salamon, Oriol Nieto, Dawen Liang, Daniel PW Ellis, and C. Colin Raffel. “mir_eval: A transparent implementation of common MIR metrics.” In In Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR. 2014.McFee, Brian, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. “librosa: Audio and music signal analysis in python.” In Proceedings of the 14th python in science conference, pp. 18-25. 2015.","categories":[],"tags":[{"name":"dnn","slug":"dnn","permalink":"nateanl.github.io/tags/dnn/"}]},{"title":"Word2vec, skip-gram, and negative sampling","slug":"word2vec","date":"2019-02-25T15:58:49.000Z","updated":"2020-03-05T14:14:49.924Z","comments":true,"path":"2019/02/25/word2vec/","link":"","permalink":"nateanl.github.io/2019/02/25/word2vec/","excerpt":"Though it’s already 9102, I stii want to talk a little about word2vec…","text":"Though it’s already 9102, I stii want to talk a little about word2vec… What is word2vec?Linear layer VS Embedding layerSkip-gram ModelNegative SamplingHow to choose negative words?Is Softmax really useful for Skip-gram model?To be continued…","categories":[],"tags":[{"name":"nlp","slug":"nlp","permalink":"nateanl.github.io/tags/nlp/"}]},{"title":"Build neural networks from scratch","slug":"neural-nets","date":"2019-02-13T01:45:40.000Z","updated":"2020-03-05T01:38:43.337Z","comments":true,"path":"2019/02/13/neural-nets/","link":"","permalink":"nateanl.github.io/2019/02/13/neural-nets/","excerpt":"Recently I read a post by Denny Britz about implementing a neural network from scratch in Python. I was pretty inspired by it. I still remember the days when I tried to study NN and it took me a bunch of hours to understand the gradients, chain rule, back-propagation, and so on. So it’s good to write a “Neural-Net for Baby” (NB) post to help myself understand it better. Okay, let’s start with a single-layer single-output neural net. We can regard the model as a linear regression model. o = \\sigma(\\sum_{i}^{N}w_ix_i + b)","text":"Recently I read a post by Denny Britz about implementing a neural network from scratch in Python. I was pretty inspired by it. I still remember the days when I tried to study NN and it took me a bunch of hours to understand the gradients, chain rule, back-propagation, and so on. So it’s good to write a “Neural-Net for Baby” (NB) post to help myself understand it better. Okay, let’s start with a single-layer single-output neural net. We can regard the model as a linear regression model. o = \\sigma(\\sum_{i}^{N}w_ix_i + b) $x$ is an N $\\times$ 1 input vector, $w$ is an N $\\times$ 1 weight vector, and $b$ is a bias scalar. $\\sigma$ is an activation function (I will talk about it later). Think in a geometry way, $w$ is the transformation matrix which projects the vector $x$ to a 1-D space. $b$ is a shift scalar (or a vector in multi-output net) which does linear-shift in the new space. Imagine you are playing golf, there are two holes: A and B. There are two kinds of balls: red and black. The golf club is the neural network. Every time you swing the club, the red ball will always fall into hole A, while the black ball will always fall into hole B. Some people tend to explain the weight value as the importance of the input feature value. Well, what if the feature value is negative and the weight is negative too? It doesn’t mean anything if the weight value is 0 also. Updating the weights is updating the bases of the hyperspace where the input vector is projected to. Back PropagationNow we know the neural network does projection (linear way or non-linear way). But how to train the network, to make it useful? Start with a simple example: o = axif $x$ is 1, $y$ is 2, $a$ is 1. So $o$ is 1. However, we want the result to be as close to $y$ as possible. So there is a difference between the output $o$ and the target $y$. We use mean square error as the loss function to compute the loss: L = (y-o)^2The gradient is $\\frac{\\partial L}{\\partial o}$ which is $2(o-y)$ which is -2. This means o needs to be larger to make the gradient decrease. Since we want to update $a$, we need to compute the gradient of a: \\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial o} \\times \\frac{\\partial o}{\\partial a} = 2(o-y)xSo the gradient for $a$ is -2, and we can update it by this formula: a = a - \\eta \\frac{\\partial L}{\\partial a}$\\eta$ is called the learning rate. We want to iteratively make small changes on the weight so that the output will finally be close to the target. Set $\\eta$ to be 0.1, the updated $a$ will be 1.2. The new output $o$ will be 1.2. The new loss will be 0.64, which is smaller than before! After several iterations, we can make the output close to the target. Multi-Layer NetworkIf the neural networks has more than one layer, how can we train the model? According to the chain rule, it is easy AF. I really like to describe it in a matrix way because you can see how beautiful the formula is. But let’s start with this example: o_2 = W_2(W_1x+b_1)+b_2o_1 = W_1x+b_1$x$ is a $M$ dimensional vector. $W_1$ is a $M \\times N$ matrix, $W_2$ is a $N \\times K$ matrix, $b_1$ is a $N$ dimensional vector, and $b_2$ is a $K$ dimensional vector. The loss function is still mean square error: L = (y-o_2)^2The gradient for $W_2$ is: \\begin{gathered} \\frac{\\partial L}{\\partial W_2^{n,k}} &= &\\frac{\\partial L}{\\partial o_2^k} \\frac{\\partial o_2^k}{\\partial W_2^{n,k}}\\\\ &= &2(o_2^k-y^k)o_1^n \\end{gathered}$n$ is the index of $o_1$. $k$ is the index of $o_2$. So the gradient of $W_2$ is a $N\\times K$ matrix: \\frac{\\partial L}{\\partial W_2} = 2 o_1^T\\times(o_2-y)The gradient for $W_1$ is: \\begin{gathered} \\frac{\\partial L}{\\partial W_1^{m,n}} &=&\\sum_k{\\frac{\\partial L}{\\partial o_2^k}} \\frac{\\partial o_2^k}{\\partial o_1^n} \\frac{\\partial o_i^n}{\\partial W_1^{m,n}}\\\\ &=&\\sum_k{2(o_2^k-y^k)W_2^{n,k}} x^m \\end{gathered}It can also be transformed to a matrix form: \\frac{\\partial L}{\\partial W_1} =2x^T(o_2-y)W_2^TAfter we got these gradient matrix, we can update those weights by using the back-propagation method. In Python you can just apply numpy to do it. W_1 = W_1 - \\eta \\frac{\\partial L}{\\partial W_1} = W_1 - 2\\eta x^T(o_2-y)W_2^TSo, in this way, we can build our neural network model by using pure Python! The only problem is whenever we create a new model, we have to create a new backward function which contains a bunch of redundant codes. Chainer is the first framework which applies the “autograd”: automatic differentiation method to calculate the gradients automatically. I won’t recommend implementing such “autograd” things from scratch (There is a Chinese slang: 人生苦短，我用PyTorch ). But it is a good start to follow Denny’s notebook and build a simple two-layer neural network from scratch, and you can understand the idea of NN in a mathematical way.","categories":[],"tags":[{"name":"dnn","slug":"dnn","permalink":"nateanl.github.io/tags/dnn/"},{"name":"pytorch","slug":"pytorch","permalink":"nateanl.github.io/tags/pytorch/"}]},{"title":"Latex Test","slug":"latex","date":"2018-12-20T17:45:34.000Z","updated":"2020-03-05T01:14:33.178Z","comments":true,"path":"2018/12/21/latex/","link":"","permalink":"nateanl.github.io/2018/12/21/latex/","excerpt":"","text":"e^{i\\pi} + 1 = 0","categories":[],"tags":[{"name":"latex","slug":"latex","permalink":"nateanl.github.io/tags/latex/"}]}]}