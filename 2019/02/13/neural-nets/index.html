<!DOCTYPE html>
<html>
  <head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
  <meta name="description" content="Zhaoheng Ni&#39;s blog">
  <meta name="keyword" content>
  
    <link rel="shortcut icon" href="/css/images/logo.svg">
  
  <title>
    
      Build neural networks from scratch | Zhaoheng Ni
    
  </title>
  <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.css" rel="stylesheet">
  <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/[object Object].min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/style.css">
  
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/geopattern/1.2.3/js/geopattern.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/nprogress/0.2.0/nprogress.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
  
  
  


</head>
<div class="wechat-share">
  <img src="/css/images/logo.svg">
</div>

  <body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header fixed-header">
  <div class="header-container">
    <a class="home-link" href="/">
      <div class="logo"></div>
      <span>Zhaoheng Ni</span>
    </a>
    <ul class="right-list">
      
        <li class="list-item">
          
            <a href="/" class="item-link">Home</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/archives/" class="item-link">Archives</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/publications/" class="item-link">Publications</a>
          
        </li>
      
        <li class="list-item">
          
            <a href="/about/" class="item-link">About</a>
          
        </li>
      
    </ul>
    <div class="menu">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </div>
    <div class="menu-mask">
      <ul class="menu-list">
        
          <li class="menu-item">
            
              <a href="/" class="menu-link">Home</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/archives/" class="menu-link">Archives</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/publications/" class="menu-link">Publications</a>
            
          </li>
        
          <li class="menu-item">
            
              <a href="/about/" class="menu-link">About</a>
            
          </li>
        
      </ul>
    </div>
  </div>
</header>

    <div id="article-banner">
  <h2>Build neural networks from scratch</h2>
  <p class="post-date">2019-02-13</p>
  <div class="arrow-down">
    <a href="javascript:;"></a>
  </div>
</div>
<main class="app-body flex-box">
  <!-- Article START -->
  <article class="post-article">
    <section class="markdown-content"><p>Recently I read a post by <a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" target="_blank" rel="noopener">Denny Britz</a> about implementing a neural network from scratch in Python. I was pretty inspired by it. I still remember the days when I tried to study NN and it took me a bunch of hours to understand the gradients, chain rule, back-propagation, and so on. So it’s good to write a “Neural-Net for Baby” (NB) post to help myself understand it better.</p>
<p>Okay, let’s start with a single-layer single-output neural net. We can regard the model as a linear regression model.</p>
<script type="math/tex; mode=display">o = \sigma(\sum_{i}^{N}w_ix_i + b)</script><a id="more"></a>
<p>$x$ is an N $\times$ 1 input vector, $w$ is an N $\times$ 1 weight vector, and $b$ is a bias scalar. $\sigma$ is an activation function (I will talk about it later).</p>
<p>Think in a geometry way, $w$ is the transformation matrix which projects the vector $x$ to a 1-D space. $b$ is a shift scalar (or a vector in multi-output net) which does linear-shift in the new space.</p>
<p>Imagine you are playing golf, there are two holes: A and B. There are two kinds of balls: red and black. The golf club is the neural network. Every time you swing the club, the red ball will always fall into hole A, while the black ball will always fall into hole B.</p>
<p>Some people tend to explain the weight value as the importance of the input feature value. Well, what if the feature value is negative and the weight is negative too? It doesn’t mean anything if the weight value is 0 also. Updating the weights is updating the bases of the hyperspace where the input vector is projected to.</p>
<h3 id="Back-Propagation"><a href="#Back-Propagation" class="headerlink" title="Back Propagation"></a>Back Propagation</h3><p>Now we know the neural network does projection (linear way or non-linear way). But how to train the network, to make it useful?</p>
<p>Start with a simple example:</p>
<script type="math/tex; mode=display">o = ax</script><p>if $x$ is 1, $y$ is 2, $a$ is 1. So $o$ is 1. However, we want the result to be as close to $y$ as possible. So there is a difference between the output $o$ and the target $y$. We use mean square error as the loss function to compute the loss:</p>
<script type="math/tex; mode=display">L = (y-o)^2</script><p>The gradient is $\frac{\partial L}{\partial o}$ which is $2(o-y)$ which is -2. This means o needs to be larger to make the gradient decrease. Since we want to update $a$, we need to compute the gradient of a:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial a} = \frac{\partial L}{\partial o} \times \frac{\partial o}{\partial a} = 2(o-y)x</script><p>So the gradient for $a$ is -2, and we can update it by this formula:</p>
<script type="math/tex; mode=display">a = a - \eta  \frac{\partial L}{\partial a}</script><p>$\eta$ is called the learning rate. We want to iteratively make small changes on the weight so that the output will finally be close to the target.</p>
<p>Set $\eta$ to be 0.1, the updated $a$ will be 1.2. The new output $o$ will be 1.2. The new loss will be 0.64, which is smaller than before! After several iterations, we can make the output close to the target.</p>
<h3 id="Multi-Layer-Network"><a href="#Multi-Layer-Network" class="headerlink" title="Multi-Layer Network"></a>Multi-Layer Network</h3><p>If the neural networks has more than one layer, how can we train the model? According to the chain rule, it is easy AF.</p>
<p>I really like to describe it in a matrix way because you can see how beautiful the formula is. But let’s start with this example:</p>
<script type="math/tex; mode=display">o_2 = W_2(W_1x+b_1)+b_2</script><script type="math/tex; mode=display">o_1 = W_1x+b_1</script><p>$x$ is a $M$ dimensional vector. $W_1$ is a $M \times N$ matrix, $W_2$ is a $N \times K$ matrix, $b_1$ is a $N$ dimensional vector, and $b_2$ is a $K$ dimensional vector. The loss function is still mean square error:</p>
<script type="math/tex; mode=display">L = (y-o_2)^2</script><p>The gradient for $W_2$ is:</p>
<script type="math/tex; mode=display">
\begin{gathered}
\frac{\partial L}{\partial W_2^{n,k}} &= &\frac{\partial L}{\partial o_2^k} \frac{\partial o_2^k}{\partial W_2^{n,k}}\\
&= &2(o_2^k-y^k)o_1^n
\end{gathered}</script><p>$n$ is the index of $o_1$. $k$ is the index of $o_2$. So the gradient of $W_2$ is a $N\times K$ matrix:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial W_2} = 2 o_1^T\times(o_2-y)</script><p>The gradient for $W_1$ is:</p>
<script type="math/tex; mode=display">
\begin{gathered}
\frac{\partial L}{\partial W_1^{m,n}} &=&\sum_k{\frac{\partial L}{\partial o_2^k}} \frac{\partial o_2^k}{\partial o_1^n} \frac{\partial o_i^n}{\partial W_1^{m,n}}\\
 &=&\sum_k{2(o_2^k-y^k)W_2^{n,k}} x^m
 \end{gathered}</script><p>It can also be transformed to a matrix form:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial W_1} =2x^T(o_2-y)W_2^T</script><p>After we got these gradient matrix, we can update those weights by using the back-propagation method. In Python you can just apply numpy to do it.</p>
<script type="math/tex; mode=display">W_1 = W_1 - \eta \frac{\partial L}{\partial W_1}
= W_1 - 2\eta x^T(o_2-y)W_2^T</script><p>So, in this way, we can build our neural network model by using pure Python! The only problem is whenever we create a new model, we have to create a new backward function which contains a bunch of redundant codes. <a href="https://chainer.org/" target="_blank" rel="noopener">Chainer</a> is the first framework which applies the “autograd”: automatic differentiation method to calculate the gradients automatically. I won’t recommend implementing such “autograd” things from scratch (There is a Chinese slang: <font color="red">人生苦短，我用PyTorch </font>). But it is a good start to follow Denny’s notebook and build a simple two-layer neural network from scratch, and you can understand the idea of NN in a mathematical way.</p>
</section>
    <!-- Tags START -->
    
      <div class="tags">
        <span>Tags:</span>
        
  <a href="/tags#dnn" >
    <span class="tag-code">dnn</span>
  </a>

  <a href="/tags#pytorch" >
    <span class="tag-code">pytorch</span>
  </a>

      </div>
    
    <!-- Tags END -->
    <!-- NAV START -->
    
  <div class="nav-container">
    <!-- reverse left and right to put prev and next in a more logic postition -->
    
      <a class="nav-left" href="/2018/12/21/latex/">
        <span class="nav-arrow">← </span>
        
          Latex Test
        
      </a>
    
    
      <a class="nav-right" href="/2019/02/25/word2vec/">
        
          Word2vec, skip-gram, and negative sampling
        
        <span class="nav-arrow"> →</span>
      </a>
    
  </div>

    <!-- NAV END -->
    <!-- 打赏 START -->
    
    <!-- 打赏 END -->
    <!-- 二维码 START -->
    
    <!-- 二维码 END -->
    
      <!-- No Comment -->
    
  </article>
  <!-- Article END -->
  <!-- Catalog START -->
  
  <!-- Catalog END -->
</main>

<script>
  (function () {
    var url = 'nateanl.github.io/2019/02/13/neural-nets/';
    var banner = 'undefined'
    if (banner !== '' && banner !== 'undefined' && banner !== 'null') {
      $('#article-banner').css({
        'background-image': 'url(' + banner + ')'
      })
    } else {
      $('#article-banner').geopattern(url)
    }
    $('.header').removeClass('fixed-header')

    // error image
    $(".markdown-content img").on('error', function() {
      $(this).attr('src', 'http://file.muyutech.com/error-img.png')
      $(this).css({
        'cursor': 'default'
      })
    })

    // zoom image
    $(".markdown-content img").on('click', function() {
      var src = $(this).attr('src')
      if (src !== 'http://file.muyutech.com/error-img.png') {
        var imageW = $(this).width()
        var imageH = $(this).height()

        var zoom = ($(window).width() * 0.95 / imageW).toFixed(2)
        zoom = zoom < 1 ? 1 : zoom
        zoom = zoom > 2 ? 2 : zoom
        var transY = (($(window).height() - imageH) / 2).toFixed(2)

        $('body').append('<div class="image-view-wrap"><div class="image-view-inner"><img src="'+ src +'" /></div></div>')
        $('.image-view-wrap').addClass('wrap-active')
        $('.image-view-wrap img').css({
          'width': `${imageW}`,
          'transform': `translate3d(0, ${transY}px, 0) scale3d(${zoom}, ${zoom}, 1)`
        })
        $('html').css('overflow', 'hidden')

        $('.image-view-wrap').on('click', function() {
          $(this).remove()
          $('html').attr('style', '')
        })
      }
    })
  })();
</script>









  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
  <script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: '62eab4c11f0968939d65',
        clientSecret: '7d54478cce99144f1291d51569f4c68a58088c34',
        repo: 'page_comments',
        owner: 'nateanl',
        admin: ['nateanl'],
        id: location.pathname,
        distractionFreeMode: true,
        language: 'en',
      })
      gitalk.render('gitalk-container')
     </script>


    <div class="scroll-top">
  <span class="arrow-icon"></span>
</div>
    <footer class="app-footer">
  <p class="copyright">
    page view <span id="busuanzi_value_site_pv"></span>,
    visitor <span id="busuanzi_value_site_uv"></span>
    <br>
    &copy; 2020 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a>
    | Theme: <a href="https://github.com/yanm1ng/hexo-theme-vexo" target="_blank">vexo</a>
  </p>
</footer>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script>
  function async(u, c) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }
</script>
<script>
  async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
    FastClick.attach(document.body);
  })
</script>

<script>
  var hasLine = 'true';
  async("//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js", function(){
    $('figure pre').each(function(i, block) {
      var figure = $(this).parents('figure');
      if (hasLine === 'false') {
        figure.find('.gutter').hide();
      }
      var lang = figure.attr('class').split(' ')[1] || 'code';
      var codeHtml = $(this).html();
      var codeTag = document.createElement('code');
      codeTag.className = lang;
      codeTag.innerHTML = codeHtml;
      $(this).attr('class', '').empty().html(codeTag);
      figure.attr('data-lang', lang.toUpperCase());
      hljs.highlightBlock(block);
    });
  })
</script>
<!-- Baidu Tongji -->

<script src="/js/script.js"></script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>