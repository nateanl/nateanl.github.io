<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>From forced alignment to acoustic model: a chicken-and-egg cycle in ASR</title>
      <link href="/2020/09/01/force-aligment/"/>
      <url>/2020/09/01/force-aligment/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>If you are familiar with <a href="https://kaldi-asr.org/" target="_blank" rel="noopener">Kaldi</a>, an open-source toolkit for speech recognition, then you must know those crazy script shits like this:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">steps/train_mono.sh  # mono-phone AM</span><br><span class="line">...</span><br><span class="line">steps/align_si.sh</span><br><span class="line">...</span><br><span class="line">steps/train_lda_mllt.sh # tri-phone AM</span><br><span class="line">...</span><br><span class="line">steps/align_si.sh</span><br><span class="line">...</span><br><span class="line">steps/train_sat.sh # tri-phone AM again</span><br></pre></td></tr></table></figure><p>Wow, why are there so many training and aligning loops, like it never stops?</p><p>If the ASR system is not an end-to-end system (i.e. from wav to text), it usually comprises an acoustic model (AM), which predicts the acoustic state of each frame, and a language model (LM), which predicts the word sequence out of the acoustic states. The acoustic model could be of any kind, for example, GMM, HMM, or DNN. But to train the acoustic model, we need to prepare the feature and label pairs. If the feature is MFCC, the label is the phonemes for the frames.</p><p>Here comes the problem, not all frames contains speech. Many of them have only noises, or silence. We need to correctly align the labels and the corresponding frames.</p><p>Do we have such accurate labels? No. We usually have the text label annotated by humans, however, they are usually in a sentence level and don’t have the precise start and end points for each phoneme. So, what should we do?</p><p>The first ASR genius who thought of the above scripts might say: Fxxk it, let’s just train it. I have the MFCC frames, and I have the phonemes, I just equally split the frames to each phoneme and feed them to the model, period.</p><p>Though the labels are not so clean, the model does learn something, which means, given a frame, the model can tell which phoneme has the highest likelihood, though it’s not always accurate.</p><p>Then the ASR genius said: wait, I can use this trained AM to do something. If it predicts the likelihood for all frames, why don’t I use the predictions to re-align the frames, and train another AM? So he did it. He aligns the frames based on the AM predictions and the text transcription. It is called “forced alignment”. Based on the pre-trained mono-phone AM, he generates new alignment labels and trains a tri-phone AM.</p><p>The tri-phone AM is more powerful than the mono-phone AM. That’s good. But the genius is not satisfied yet. Since we have a better AM, we can get better alignment results. Why not run another round of training, and get a better AM?</p><p>So you can see the trick here. If we want a better acoustic model, we need to prepare better alignments, but if we want better alignments, we need to train a better acoustic model. It’s a chicken-and-egg problem! But don’t worry, once we have a start (the mono AM), we are almost succeeding. We just keep looping the training and aligning process, like the EM algorithm.</p><p>We have talked about the relationship between forced alignment and acoustic model training. Force alignment is very useful especially in the multi-talker scenario. Now I want to ask you a question: What if have nothing but the wav files when we run the decoding for the multi-talker audio? Next time, I will talk about the methods to solve this problem…</p>]]></content>
      
      
      
        <tags>
            
            <tag> asr </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Onssen: an open-source speech separation and enhancement library</title>
      <link href="/2020/03/05/onssen/"/>
      <url>/2020/03/05/onssen/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>We are happy to release <code>onssen</code> (pronounced as 温泉, the Japanese spring), an PyTorch-based deep learning library for speech enhancement and speech separation. Feel free to fork it on <a href="https://github.com/speechLabBcCuny/onssen" target="_blank" rel="noopener">GitHub</a> and add your models to it :)</p><h2 id="It-supports-the-following-separation-models"><a href="#It-supports-the-following-separation-models" class="headerlink" title="It supports the following separation models:"></a>It supports the following separation models:</h2><ul><li>Deep Clustering</li><li>Chimera Net</li><li>Chimera++</li><li>Phase Estimation Network</li><li>Speech Enhancement with Restoration Layers</li></ul><h2 id="Supported-Dataset"><a href="#Supported-Dataset" class="headerlink" title="Supported Dataset"></a>Supported Dataset</h2><ul><li>Wsj0-2mix (<a href="http://www.merl.com/demos/deep-clustering" target="_blank" rel="noopener">http://www.merl.com/demos/deep-clustering</a>)</li><li>Daps (<a href="https://archive.org/details/daps_dataset" target="_blank" rel="noopener">https://archive.org/details/daps_dataset</a>)</li><li>Edinburgh-TTS (<a href="https://datashare.is.ed.ac.uk/handle/10283/2791" target="_blank" rel="noopener">https://datashare.is.ed.ac.uk/handle/10283/2791</a>)</li></ul><h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h2><ul><li>PyTorch</li><li>LibRosa</li><li>NumPy</li></ul><h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><p>You can simply use the existing config JSON file or customize your config file to train the enhancement or separation model.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py -c configs/dc_config.json</span><br></pre></td></tr></table></figure><h2 id="Citing"><a href="#Citing" class="headerlink" title="Citing"></a>Citing</h2><p>If you plan to use <code>onssen</code> for your research project, please cite one of the following bibtex citations:</p><pre><code>@inproceedings {onssen,    author = {Zhaoheng Ni and Michael Mandel},    title = &quot;ONSSEN: An Open-source Speech Separation and Enhancement Library&quot;,    publisher = &quot;under review&quot;,    year = 2019}@Misc{onssen,    author = {Zhaoheng Ni and Michael Mandel},    title = &quot;ONSSEN: An Open-source Speech Separation and Enhancement Library&quot;,    howpublished = {\url{https://github.com/speechLabBcCuny/onssen}},    year =        {2019}}</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> dnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mask-dependent phase estimation for monaural speaker separation</title>
      <link href="/2019/04/25/speech-separation-demo/"/>
      <url>/2019/04/25/speech-separation-demo/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Speech separation refers to the task of isolating speech of interest in a multi-talker environment.</p><a id="more"></a><p>Most methods apply real-valued Time-Frequency (T-F) Masks to the Short-time Fourier Transform (STFT) of the noisy speech to get the estimated clean speech. The Idea Ratio Mask (IRM) is defined as</p><script type="math/tex; mode=display">IRM = \frac{|S|}{|X|},</script><p>where $|S|$ and $|Y|$ is the magnitude of clean speech and noisy speech respectively.</p><p align="center"> <img src="/images/mix.png" width="600"> </p><p>The mask value is close to 1 if the T-F bin is dominated by the speaker, 0 otherwise.</p><p align="center"> <img src="/images/irm.png" width="600"> </p><p>Since the clean STFT estimate is constructed by $IRM \odot X$, there exists unavoidable phase difference in the masking method.</p><p>We propose a novel phase estimation method which estimates the phase based on the T-F mask. Now the code is included in the <a href="https://github.com/speechLabBcCuny/onssen" target="_blank" rel="noopener"><code>onssen</code></a> library.</p><p>To evaluate the separation performance of our method, we compare our results with recent published ones including:</p><ul><li>Deep Clustering</li><li>Chimera++ Network with MSA loss function</li><li>Chimera++ Network with tPSA loss function</li></ul><p>Here are the separation demos for different methods. The testing utterance is from WSJ0-2mix dataset.</p><center><table>    <thead>    <tr>  <th align="left">Mixture Utterance</th>      </tr><th align="center">tt/mix/050a0508_1.4465_22gc0102_-1.4465.wav</th>        <tr>    <th><audio controls="controls" style="width: 230px;"><source type="audio/mp3" src="/audios/mixture.mp3"></audio> </th>  </tr>  </thead>  <tbody>  </tbody> </table> </center><h3 id="Separation-Results-of-different-models"><a href="#Separation-Results-of-different-models" class="headerlink" title="Separation Results of different models"></a>Separation Results of different models</h3><h3 id="Swipe-left-to-listen-to-the-audios"><a href="#Swipe-left-to-listen-to-the-audios" class="headerlink" title="(Swipe left to listen to the audios)"></a>(Swipe left to listen to the audios)</h3><table>    <thead>        <tr>            <th>Model</th> <th>Average SDR</th><th align="center">Speaker 1</th> <th align="center">Speaker 2</th>        </tr>        <tr>           <th> Deep Clustering</th> <th>9.2 dB</th><th><audio controls="controls" style="width: 230px;"><source type="audio/mp3" src="/audios/dc_s1.mp3"></audio> </th> <th><audio controls="controls" style="width: 230px;"><source type="audio/mp3" src="/audios/dc_s2.mp3"></audio> </th>        </tr>        <tr>           <th> Chimera++ MSA</th> <th>10.2 dB</th><th><audio controls="controls" style="width: 230px;"><source type="audio/mp3" src="/audios/chimera_msa_s1.mp3"></audio> </th> <th><audio controls="controls" style="width: 230px;"><source type="audio/mp3" src="/audios/chimera_msa_s2.mp3"></audio> </th>        </tr>        <tr>        <th> Chimera++ tPSA</th> <th>10.3 dB</th><th><audio controls="controls" style="width: 230px;"><source type="audio/mp3" src="/audios/chimera_tpsa_s1.mp3"></audio> </th> <th><audio controls="controls" style="width: 230px;"><source type="audio/mp3" src="/audios/chimera_tpsa_s2.mp3"></audio> </th>        </tr>        <tr>        <th>Phase Estimation</th> <th>13.6 dB</th><th><audio controls="controls" style="width: 230px;"><source type="audio/mp3" src="/audios/phase_s2.mp3"></audio> </th> <th><audio controls="controls" style="width: 230px;"><source type="audio/mp3" src="/audios/phase_s1.mp3"></audio> </th>        </tr>      <tr>        <th>Clean Reference</th> <th>301 dB (basically $+\infty$)</th><th><audio controls="controls" style="width: 230px;"><source type="audio/mp3" src="/audios/ref_s2.mp3"></audio> </th> <th><audio controls="controls" style="width: 230px;"><source type="audio/mp3" src="/audios/ref_s1.mp3"></audio> </th>        </tr>    </thead>    <tbody>    </tbody></table><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>Hershey, John R., Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe. “Deep clustering: Discriminative embeddings for segmentation and separation.” In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 31-35. IEEE, 2016.<br>Yu, Dong, Morten Kolbæk, Zheng-Hua Tan, and Jesper Jensen. “Permutation invariant training of deep models for speaker-independent multi-talker speech separation.” In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 241-245. IEEE, 2017.<br>Wang, Zhong-Qiu, Jonathan Le Roux, and John R. Hershey. “Alternative objective functions for deep clustering.” In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 686-690. IEEE, 2018.<br>Wang, Zhong-Qiu, Jonathan Le Roux, DeLiang Wang, and John R. Hershey. “End-to-end speech separation with unfolded iterative phase reconstruction.” arXiv preprint arXiv:1804.10204 (2018).<br>Paszke, Adam, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. “Automatic differentiation in pytorch.” (2017).<br>Raffel, Colin, Brian McFee, Eric J. Humphrey, Justin Salamon, Oriol Nieto, Dawen Liang, Daniel PW Ellis, and C. Colin Raffel. “mir_eval: A transparent implementation of common MIR metrics.” In In Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR. 2014.<br>McFee, Brian, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. “librosa: Audio and music signal analysis in python.” In Proceedings of the 14th python in science conference, pp. 18-25. 2015.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2vec, skip-gram, and negative sampling</title>
      <link href="/2019/02/25/word2vec/"/>
      <url>/2019/02/25/word2vec/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Though it’s already 9102, I stii want to talk a little about word2vec…</p><a id="more"></a><h3 id="What-is-word2vec"><a href="#What-is-word2vec" class="headerlink" title="What is word2vec?"></a>What is word2vec?</h3><h3 id="Linear-layer-VS-Embedding-layer"><a href="#Linear-layer-VS-Embedding-layer" class="headerlink" title="Linear layer VS Embedding layer"></a>Linear layer VS Embedding layer</h3><h3 id="Skip-gram-Model"><a href="#Skip-gram-Model" class="headerlink" title="Skip-gram Model"></a>Skip-gram Model</h3><h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><h3 id="How-to-choose-negative-words"><a href="#How-to-choose-negative-words" class="headerlink" title="How to choose negative words?"></a>How to choose negative words?</h3><h3 id="Is-Softmax-really-useful-for-Skip-gram-model"><a href="#Is-Softmax-really-useful-for-Skip-gram-model" class="headerlink" title="Is Softmax really useful for Skip-gram model?"></a>Is Softmax really useful for Skip-gram model?</h3><p>To be continued…</p>]]></content>
      
      
      
        <tags>
            
            <tag> nlp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Build neural networks from scratch</title>
      <link href="/2019/02/13/neural-nets/"/>
      <url>/2019/02/13/neural-nets/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Recently I read a post by <a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" target="_blank" rel="noopener">Denny Britz</a> about implementing a neural network from scratch in Python. I was pretty inspired by it. I still remember the days when I tried to study NN and it took me a bunch of hours to understand the gradients, chain rule, back-propagation, and so on. So it’s good to write a “Neural-Net for Baby” (NB) post to help myself understand it better.</p><p>Okay, let’s start with a single-layer single-output neural net. We can regard the model as a linear regression model.</p><script type="math/tex; mode=display">o = \sigma(\sum_{i}^{N}w_ix_i + b)</script><a id="more"></a><p>$x$ is an N $\times$ 1 input vector, $w$ is an N $\times$ 1 weight vector, and $b$ is a bias scalar. $\sigma$ is an activation function (I will talk about it later).</p><p>Think in a geometry way, $w$ is the transformation matrix which projects the vector $x$ to a 1-D space. $b$ is a shift scalar (or a vector in multi-output net) which does linear-shift in the new space.</p><p>Imagine you are playing golf, there are two holes: A and B. There are two kinds of balls: red and black. The golf club is the neural network. Every time you swing the club, the red ball will always fall into hole A, while the black ball will always fall into hole B.</p><p>Some people tend to explain the weight value as the importance of the input feature value. Well, what if the feature value is negative and the weight is negative too? It doesn’t mean anything if the weight value is 0 also. Updating the weights is updating the bases of the hyperspace where the input vector is projected to.</p><h3 id="Back-Propagation"><a href="#Back-Propagation" class="headerlink" title="Back Propagation"></a>Back Propagation</h3><p>Now we know the neural network does projection (linear way or non-linear way). But how to train the network, to make it useful?</p><p>Start with a simple example:</p><script type="math/tex; mode=display">o = ax</script><p>if $x$ is 1, $y$ is 2, $a$ is 1. So $o$ is 1. However, we want the result to be as close to $y$ as possible. So there is a difference between the output $o$ and the target $y$. We use mean square error as the loss function to compute the loss:</p><script type="math/tex; mode=display">L = (y-o)^2</script><p>The gradient is $\frac{\partial L}{\partial o}$ which is $2(o-y)$ which is -2. This means o needs to be larger to make the gradient decrease. Since we want to update $a$, we need to compute the gradient of a:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial a} = \frac{\partial L}{\partial o} \times \frac{\partial o}{\partial a} = 2(o-y)x</script><p>So the gradient for $a$ is -2, and we can update it by this formula:</p><script type="math/tex; mode=display">a = a - \eta  \frac{\partial L}{\partial a}</script><p>$\eta$ is called the learning rate. We want to iteratively make small changes on the weight so that the output will finally be close to the target.</p><p>Set $\eta$ to be 0.1, the updated $a$ will be 1.2. The new output $o$ will be 1.2. The new loss will be 0.64, which is smaller than before! After several iterations, we can make the output close to the target.</p><h3 id="Multi-Layer-Network"><a href="#Multi-Layer-Network" class="headerlink" title="Multi-Layer Network"></a>Multi-Layer Network</h3><p>If the neural networks has more than one layer, how can we train the model? According to the chain rule, it is easy AF.</p><p>I really like to describe it in a matrix way because you can see how beautiful the formula is. But let’s start with this example:</p><script type="math/tex; mode=display">o_2 = W_2(W_1x+b_1)+b_2</script><script type="math/tex; mode=display">o_1 = W_1x+b_1</script><p>$x$ is a $M$ dimensional vector. $W_1$ is a $M \times N$ matrix, $W_2$ is a $N \times K$ matrix, $b_1$ is a $N$ dimensional vector, and $b_2$ is a $K$ dimensional vector. The loss function is still mean square error:</p><script type="math/tex; mode=display">L = (y-o_2)^2</script><p>The gradient for $W_2$ is:</p><script type="math/tex; mode=display">\begin{gathered}\frac{\partial L}{\partial W_2^{n,k}} &= &\frac{\partial L}{\partial o_2^k} \frac{\partial o_2^k}{\partial W_2^{n,k}}\\&= &2(o_2^k-y^k)o_1^n\end{gathered}</script><p>$n$ is the index of $o_1$. $k$ is the index of $o_2$. So the gradient of $W_2$ is a $N\times K$ matrix:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial W_2} = 2 o_1^T\times(o_2-y)</script><p>The gradient for $W_1$ is:</p><script type="math/tex; mode=display">\begin{gathered}\frac{\partial L}{\partial W_1^{m,n}} &=&\sum_k{\frac{\partial L}{\partial o_2^k}} \frac{\partial o_2^k}{\partial o_1^n} \frac{\partial o_i^n}{\partial W_1^{m,n}}\\ &=&\sum_k{2(o_2^k-y^k)W_2^{n,k}} x^m \end{gathered}</script><p>It can also be transformed to a matrix form:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial W_1} =2x^T(o_2-y)W_2^T</script><p>After we got these gradient matrix, we can update those weights by using the back-propagation method. In Python you can just apply numpy to do it.</p><script type="math/tex; mode=display">W_1 = W_1 - \eta \frac{\partial L}{\partial W_1}= W_1 - 2\eta x^T(o_2-y)W_2^T</script><p>So, in this way, we can build our neural network model by using pure Python! The only problem is whenever we create a new model, we have to create a new backward function which contains a bunch of redundant codes. <a href="https://chainer.org/" target="_blank" rel="noopener">Chainer</a> is the first framework which applies the “autograd”: automatic differentiation method to calculate the gradients automatically. I won’t recommend implementing such “autograd” things from scratch (There is a Chinese slang: <font color="red">人生苦短，我用PyTorch </font>). But it is a good start to follow Denny’s notebook and build a simple two-layer neural network from scratch, and you can understand the idea of NN in a mathematical way.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dnn </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Latex Test</title>
      <link href="/2018/12/21/latex/"/>
      <url>/2018/12/21/latex/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script type="math/tex; mode=display">e^{i\pi} + 1 = 0</script>]]></content>
      
      
      
        <tags>
            
            <tag> latex </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
