<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<title>
  
    Archive: 2019/2
  
</title>

<meta property="og:type" content="website">
<meta property="og:title" content="Zhaoheng Ni">
<meta property="og:url" content="https://nizhaoheng.com/archives/2019/02/index.html">
<meta property="og:site_name" content="Zhaoheng Ni">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Zhaoheng Ni">


  <link rel="alternative" href="/atom.xml" title="Zhaoheng Ni" type="application/atom+xml">



  <link rel="icon" href="favicon.ico">


<link rel="stylesheet" href="/perfect-scrollbar/css/perfect-scrollbar.min.css">
<link rel="stylesheet" href="/styles/main.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->





</head>
<body class="monochrome">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header">
  <button class="sidebar-toggle" type="button">
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
    <span class="icon-bar"></span>
  </button>
  <a class="title" href="/">Zhaoheng Ni</a>
</div>

  <div class="main-container">
    <div class="sidebar">
  <div class="header">
    <h1 class="title"><a href="/">Zhaoheng Ni</a></h1>
    
    <div class="info">
      <div class="content">
        
        
      </div>
      
        <div class="avatar">
          
            <a href="/about"><img src="https://scontent-lga3-1.xx.fbcdn.net/v/t1.0-9/12809714_855836737873093_5654240968234810111_n.jpg?_nc_cat=101&amp;_nc_ht=scontent-lga3-1.xx&amp;oh=441243d7a0c916b96744e12290e61f90&amp;oe=5CA1F21C"></a>
          
        </div>
      
    </div>
  </div>
  <div class="body">
    
      
        <ul class="nav">
          
            
              <li class="category-list-container">
                <a href="javascript:;">Category</a>
                
              </li>
            
          
            
              <li class="tag-list-container">
                <a href="javascript:;">Tag</a>
                <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/dnn/">dnn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/">pytorch</a><span class="tag-list-count">1</span></li></ul>
              </li>
            
          
            
              <li class="archive-list-container">
                <a href="javascript:;">Archive</a>
                <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">1</span></li></ul>
              </li>
            
          
        </ul>
      
        <ul class="nav">
          
            
              <li>
                <a href="/" title="Homepage">Homepage</a>
              </li>
            
          
            
              <li>
                <a href="/about" title="About Me">About Me</a>
              </li>
            
          
            
              <li>
                <a href="/publications" title="Publications">Publications</a>
              </li>
            
          
            
              <li>
                <a href="/archives" title="By Year">By Year</a>
              </li>
            
          
            
              <li>
                <a href="https://github.com/nateanl" title="Github" target="_blank" rel="noopener">Github</a>
              </li>
            
          
        </ul>
      
    
  </div>
</div>

    <div class="main-content">
      
          

  
  
    
      
      
      <section class="archives-wrap">
        <div class="archive-year-wrap">
          <h1><a href="/archives/2019" class="archive-year">2019</a></h1>
        </div>
        <div class="post-list">
    
  

    <div class="post-list-item article">
      <h3 class="article-header">
        <a href="/2019/02/13/neural-nets/">
  Build Neural Networks from Scratch
</a>

      </h3>
      

      <div class="article-info">
        <a href="/2019/02/13/neural-nets/"><span class="article-date">
  2019-02-13
</span>
</a>
        

        
	<span class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dnn/">dnn</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/">pytorch</a></li></ul>
	</span>


      </div>
      <div class="article-entry">
        
          <p>Recently I read a post by <a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" target="_blank" rel="noopener">Denny Britz</a> about implementing a neural network from scratch in Python. I was pretty inspired by it. I still remember the days when I tried to study NN and it took me a bunch of hours to understand the gradients, chain rule, back-propagation, and so on. So it’s good to write a “Neural-Net for Baby” (NB) post to help myself understand it better.</p>
<p>Okay, start with one-layer one-output neural net. We can regard the model as a linear regression model.</p>
<script type="math/tex; mode=display">o = \sigma(\sum_{i}^{N}w_ix_i + b)</script><p>$x$ is a N $\times$ 1 input vector, $w$ is a N $\times$ 1 weight vector, and $b$ is a bias scatter. $\sigma$ is an activation function (we will talk about it later).</p>
<p>Think in a geometry way, $w$ is the transformation matrix which projects the vector $x$ to a 1-D space. $b$ is the shift scatter (or vector in multi-units cases) which shifts the output in the new space.</p>
<p>Imagine you are playing golf, there are two holes: A and B. There are two kinds of balls: red and black. The golf club is the neural network. Every time you swing the club, the red ball will always falls into hole A, while the black ball will always falls into hole B.</p>
<p>Some people tends to explain the weight value as the importance of the input feature value. What if the feature value is negative and the weight is negative too? It doesn’t mean anything if the weight value is 0 also. In fact updating the weights is updating the bases of the hyperspace, the space where the input vector is projected to.</p>
<h3 id="Back-Propagation"><a href="#Back-Propagation" class="headerlink" title="Back Propagation"></a>Back Propagation</h3><p>Now we know neural network is basically doing projection (linear way or non-linear way). But how to train the network, to make it useful?</p>
<p>Start with a simple example: $o = ax$. if $x$ is 1, $y$ is 2, $a$ is 1. $o$ is 1, but we want the result as close to $y$ as possible. So there is a difference between the output $o$ and the target $y$. We use mean square error as the loss function to compute the loss:</p>
<script type="math/tex; mode=display">L = (y-o)^2</script><p>The gradient is $\frac{\partial L}{\partial o}$ which is $2(o-y)$ which is -2. This means o needs to be larger to make the gradient decrease. Since we want to update $a$, we need to compute the gradient of a:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial a} = \frac{\partial L}{\partial o} \times \frac{\partial o}{\partial a} = 2(o-y)x</script><p>So the gradient for $a$ is -2, and we can update it by this formula:</p>
<script type="math/tex; mode=display">a = a - \eta  \frac{\partial L}{\partial a}</script><p>$\eta$ is called learning rate, because we want to gradually make small changes on the weight so that the output will finally be close to the target.</p>
<p>Set $\eta$ to be 0.1, the updated $a$ will be 1.2. The new output $o$ will be 1.2. The new loss will be 0.64, which is smaller than before! After several iterations, we can make the output close to the target.</p>
<h3 id="Multi-Layer-Network"><a href="#Multi-Layer-Network" class="headerlink" title="Multi-Layer Network"></a>Multi-Layer Network</h3><p>If the neural networks has more than one layer, how can we train the model? According to the chain rule, it is easy AF.</p>
<p>I really like to describe it in a matrix way because you can see how beautiful the formula is. But let’s start with this example:</p>
<script type="math/tex; mode=display">o_2 = W_2(W_1x+b_1)+b_2</script><script type="math/tex; mode=display">o_1 = W_1x+b_1</script><p>$x$ is a $M$ dimensional vector. $W_1$ is a $M \times N$ matrix, $W_2$ is a $N \times K$ matrix, $b_1$ is a $N$ dimensional vector, and $b_2$ is a $K$ dimensional vector. The loss function is still mean square error:</p>
<script type="math/tex; mode=display">L = (y-o_2)^2</script><p>The gradient for $W_2$ is:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial W_2^{n,k}} = \frac{\partial L}{\partial o_2^k} \frac{\partial o_2^k}{\partial W_2^{n,k}} = 2(o_2^k-y^k)o_1^n</script><p>$n$ is the index of $o_1$. $k$ is the index of $o_2$. So the gradient of $W_2$ is a $N\times K$ matrix:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial W_2} = 2 o_1^T\times(o_2-y)</script><p>The gradient for $W_1$ is:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial W_1^{m,n}} = \sum_k{\frac{\partial L}{\partial o_2^k}} \frac{\partial o_2^k}{\partial o_1^n} \frac{\partial o_i^n}{\partial W_1^{m,n}}
= \sum_k{2(o_2^k-y^k)W_2^{n,k}} x^m</script><p>It can also be transformed to a matrix form:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial W_1} =2x^T(o_2-y)W_2^T</script><p>After we got these gradient matrix, we can update those weights by using the back-propagation method. In Python you can just apply numpy to do it.</p>
<script type="math/tex; mode=display">W_1 = W_1 - \eta \frac{\partial L}{\partial W_1}
= W_1 - 2\eta x^T(o_2-y)W_2^T</script><p>So, in this way, we can build our neural network model by using pure Python! The only problem is whenever we create a new model, we have to create a new backward function which contains a bunch of redundant codes. <a href="https://chainer.org/" target="_blank" rel="noopener">Chainer</a> is the first framework which applies the “autograd”: automatic differentiation method to calculate the gradients automatically. I won’t recommend building such autograd things from scratch (There is a Chinese slang: <font color="red">人生苦短，我用PyTorch </font>). But it is a good start to follow Denny’s notebook and build a simple two-layer neural network from scratch, and you can understand the idea of NN in a mathematical way.</p>

        
      </div>
    </div>

  




</div></section>

          <div class="main-footer">
  
    © 2019 Zhaoheng Ni
  
</div>
      
    </div>
  </div>
  <script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>

  <link rel="stylesheet" href="/PhotoSwipe/photoswipe.css">
  <link rel="stylesheet" href="/PhotoSwipe/default-skin/default-skin.css">

  <!-- Root element of PhotoSwipe. Must have class pswp. -->
  <div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
             It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

      <!-- Container that holds slides.
                PhotoSwipe keeps only 3 of them in the DOM to save memory.
                Don't modify these 3 pswp__item elements, data is added later on. -->
      <div class="pswp__container">
        <div class="pswp__item"></div>
        <div class="pswp__item"></div>
        <div class="pswp__item"></div>
      </div>

      <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
      <div class="pswp__ui pswp__ui--hidden">

        <div class="pswp__top-bar">

          <!--  Controls are self-explanatory. Order can be changed. -->

          <div class="pswp__counter"></div>

          <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

          <button class="pswp__button pswp__button--share" title="Share"></button>

          <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

          <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

          <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
          <!-- element will get class pswp__preloader--active when preloader is running -->
          <div class="pswp__preloader">
            <div class="pswp__preloader__icn">
              <div class="pswp__preloader__cut">
                <div class="pswp__preloader__donut"></div>
              </div>
            </div>
          </div>
        </div>

        <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
          <div class="pswp__share-tooltip"></div>
        </div>

        <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>

        <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button>

        <div class="pswp__caption">
          <div class="pswp__caption__center"></div>
        </div>
      </div>
    </div>
  </div>

  <script src="/PhotoSwipe/photoswipe.js"></script>
  <script src="/PhotoSwipe/photoswipe-ui-default.js"></script>


<script src="/perfect-scrollbar/js/min/perfect-scrollbar.min.js"></script>
<script src="/scripts/main.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
